<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>《Spark基础环境配置》 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一·基础环境1.虚拟机IP分配 开启虚拟机，连接FinalShell输入以下命令查看主机主机名、host映射、防火墙状态（1）cat &#x2F;etc&#x2F;hostname（2）cat &#x2F;etc&#x2F;hosts（3）systemctl status firewalld.service2.安装JDK（1）上传JDK压缩包到export&#x2F;server&#x2F;路径下（2）解压安装包  tar zxvf jdk-8u65-li">
<meta property="og:type" content="website">
<meta property="og:title" content="《Spark基础环境配置》">
<meta property="og:url" content="http://example.com/%E3%80%8ASpark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E3%80%8B/index-1.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="一·基础环境1.虚拟机IP分配 开启虚拟机，连接FinalShell输入以下命令查看主机主机名、host映射、防火墙状态（1）cat &#x2F;etc&#x2F;hostname（2）cat &#x2F;etc&#x2F;hosts（3）systemctl status firewalld.service2.安装JDK（1）上传JDK压缩包到export&#x2F;server&#x2F;路径下（2）解压安装包  tar zxvf jdk-8u65-li">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-05-23T13:56:45.000Z">
<meta property="article:modified_time" content="2022-05-23T14:04:37.514Z">
<meta property="article:author" content="gaoqi">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="page-" class="article article-type-page" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/%E3%80%8ASpark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E3%80%8B/index-1.html" class="article-date">
  <time datetime="2022-05-23T13:56:45.000Z" itemprop="datePublished">2022-05-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      《Spark基础环境配置》
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>一·基础环境<br>1.虚拟机IP分配</p>
<p>开启虚拟机，连接FinalShell输入以下命令查看主机主机名、host映射、防火墙状态<br>（1）cat /etc/hostname<br>（2）cat /etc/hosts<br>（3）systemctl status firewalld.service<br>2.安装JDK<br>（1）上传JDK压缩包到export/server/路径下<br>（2）解压安装包  tar zxvf jdk-8u65-linux-x64.tar.gz<br>建立软链接<br>ln -s /export/server/jdk-8u65-linux-x64/ /export/server/jdk<br>（3）配置环境变量 vim /etc/export 添加以下内容<br> export JAVA_HOME=/export/server/jdk1.8.0_241<br> export PATH=$PATH:$JAVA_HOME/bin<br> export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>（4）source /etc/profile 使环境变量生效</p>
<p>（5）向node2、node3分发JDK<br>scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/<br>scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/<br>(6)向node2、node3复制环境变量<br>scp /etc/profile root@node2:/etc/<br>scp /etc/profile root@node3:/etc/<br>(7)source /etc/profile 使环境变量生效<br>(8)分别在三台虚拟机执行以下命令检验安装是否成功<br>java  -version 输出以下内容代表安装成功<br>java version “1.8.0_241”<br>Java(TM) SE Runtime Environment (build 1.8.0_241-b07)<br>Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)<br>3.安装配置Hadoop集群<br>(1)集群角色规划</p>
<p>(2)服务器基础环境准备<br>主机名Vim  /etc/hostname<br>Host映射    </p>
<p>(3)关闭防火墙（3台机器）<br>systemctl stop firewalld.service 关闭防火墙<br>systemctl disable firewalld.service 禁止防火墙自动开启<br>配置SSH免密登陆<br>node1生成公钥私钥 (一路回车)<br>ssh-keygen<br>配置免密登录到node1 node2 node3<br>ssh-copy-id node1<br>ssh-copy-id node2<br>ssh-copy-id node3<br>(4)上传hadoop压缩包到export/server/路径下<br>解压 tar zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz<br>建立软链接<br>ln -s /export/server/ hadoop-3.3.0-Centos7-64 /export/server/hadoop<br>(5)修改配置文件hadoop-env.sh（配置文件路径 hadoop-3.3.0/etc/hadoop）<br>添加以下内容： export JAVA_HOME=/export/server/jdk1.8.0_241<br>    export HDFS_NAMENODE_USER=root<br>    export HDFS_DATANODE_USER=root<br>    export HDFS_SECONDARYNAMENODE_USER=root<br>    export YARN_RESOURCEMANAGER_USER=root<br>export YARN_NODEMANAGER_USER=root<br>(6)修改配置文件core-site.xml<br>添加以下内容： <!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --><br>    <property><br>        <name>fs.defaultFS</name><br>        <value>hdfs://node1:8020</value><br>    </property></p>
<pre><code>&lt;!-- 设置Hadoop本地保存数据路径 --&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 设置HDFS web UI用户身份 --&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;
</code></pre>
<p>   <value>root</value><br>    </property></p>
<pre><code>&lt;!-- 整合hive 用户代理设置 --&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;
    &lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 文件系统垃圾桶保存时间 --&gt;
&lt;property&gt;
    &lt;name&gt;fs.trash.interval&lt;/name&gt;
    &lt;value&gt;1440&lt;/value&gt;
</code></pre>
</property>
(7)修改配置文件hdfs-site.xml
添加以下内容：  <!-- 设置SNN进程运行机器位置信息 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node2:9868</value>
</property>
(8)修改配置文件 mapred-site.xml
添加以下内容：<!-- 设置MR程序默认运行模式： yarn集群模式 local本地模式 -->
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
    
<pre><code>&lt;!-- MR程序历史服务地址 --&gt;
&lt;property&gt;
</code></pre>
<p><name>mapreduce.jobhistory.address</name><br>      <value>node1:10020</value><br>    </property></p>
<pre><code>&lt;!-- MR程序历史服务器web端地址 --&gt;
&lt;property&gt;
  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
  &lt;value&gt;node1:19888&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;
  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.map.env&lt;/name&gt;
  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;
  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;
</code></pre>
</property>
(9)修改配置文件yarn-site.xml
添加以下内容：<!-- 设置YARN集群主角色运行机器位置 -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>node1</value>
    </property>
    
<pre><code>&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<!-- 是否将对容器实施物理内存限制 -->
<pre><code>&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
    &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 开启日志聚集 --&gt;
&lt;property&gt;
  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 设置yarn历史服务器地址 --&gt;
&lt;property&gt;
    &lt;name&gt;yarn.log.server.url&lt;/name&gt;
    &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;
&lt;/property&gt;

&lt;!-- 历史日志保存的时间 7天 --&gt;
&lt;property&gt;
  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
  &lt;value&gt;604800&lt;/value&gt;
</code></pre>
</property>
(10)修改配置文件workers
添加以下内容：
node1.itcast.cn
node2.itcast.cn
node3.itcast.cn

<p>(11)分发同步hadoop安装包<br>scp -r hadoop-3.3.0 root@node2:$PWD<br>scp -r hadoop-3.3.0 root@node3:$PWD<br>(12)将hadoop添加到环境变量<br>vim /etc/profile<br> export HADOOP_HOME=/export/server/hadoop-3.3.0<br>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br>(13)向node2、node3复制环境变量<br>scp /etc/profile root@node2:/etc/<br>scp /etc/profile root@node3:/etc/<br>source /etc/profile 使环境变量生效<br>(14)Hadoop集群启动<br>（==首次启动==）格式化namenode<br> hdfs namenode -format<br>脚本一键启动：start-dfs.sh 、start-yarn.sh<br>(15)访问WEB UI页面验证是否配置成功<br>HDFS集群：<a target="_blank" rel="noopener" href="http://node1:9870/">http://node1:9870/</a><br>YARN集群：<a target="_blank" rel="noopener" href="http://node1:8088/">http://node1:8088/</a><br>如图：</p>
<p>4.zookeeper集群配置<br>(1)基本规划</p>
<p>(2)上传zookeeper压缩包export/server/路径下<br>解压 tar -zxvf zookeeper-3.4.6.tar.gz<br>建立软连接<br>ln -s /export/server/zookeeper-3.4.6 /export/server/zookeeper<br>(3)修改配置文件zoo_sample.cfg<br>cp zoo_sample.cfg zoo.cfg<br>mkdir -p /export/server/zookeeper/zkdatas/<br>vim  zoo.cfg<br>修改以下内容：<br>#Zookeeper的数据存放目录<br>dataDir=/export/server/zookeeper/zkdatas</p>
<h1 id="保留多少个快照"><a href="#保留多少个快照" class="headerlink" title="保留多少个快照"></a>保留多少个快照</h1><p>autopurge.snapRetainCount=3</p>
<h1 id="日志多少小时清理一次"><a href="#日志多少小时清理一次" class="headerlink" title="日志多少小时清理一次"></a>日志多少小时清理一次</h1><p>autopurge.purgeInterval=1</p>
<h1 id="集群中服务器地址"><a href="#集群中服务器地址" class="headerlink" title="集群中服务器地址"></a>集群中服务器地址</h1><p>server.1=node1:2888:3888<br>server.2=node2:2888:3888<br>server.3=node3:2888:3888<br>(4)添加myid配置<br>echo 1 &gt; /export/server/zookeeper/zkdatas/myid<br>(5)分发安装包并修改myid值<br>cd /export/server/<br>scp -r /export/server/zookeeper-3.4.6/ node2:$PWD<br>scp -r /export/server/zookeeper-3.4.6/ node3:$PWD<br>第二台机器上建立软连接, 并修改myid的值为2<br>cd /export/server/<br>ln -s zookeeper-3.4.6/ zookeeper<br>echo 2 &gt; /export/server/zookeeper/zkdatas/myid<br>第三台机器上建立软连接, 并修改myid的值为3<br>cd /export/server/<br>ln -s zookeeper-3.4.6/ zookeeper<br>echo 3 &gt; /export/server/zookeeper/zkdatas/myid<br>(6)配置环境变量<br>vim /etc/profile<br>export ZOOKEEPER_HOME=/export/server/zookeeper<br>export PATH=$PATH:$ZOOKEEPER_HOME/bin<br>(7)向node2、node3复制环境变量<br>scp /etc/profile root@node2:/etc/<br>scp /etc/profile root@node3:/etc/<br>source /etc/profile 使环境变量生效<br>(8)三台机器启动zookeeper服务<br>/export/server/zookeeper/bin/zkServer.sh start<br>/export/server/zookeeper/bin/zkServer.sh stop<br>(9)查看服务状态<br> /export/server/zookeeper/bin/zkServer.sh  status</p>
<p>5.spark-local配置<br>(1)角色分配<br>node1: Master(HDFS\YARN\Spark) 和 Worker(HDFS\ YARN\ Spark)<br>node2: Worker(HDFS\ YARN\ Spark)<br>node3: Worker(HDFS\ YARN\ Spark) 和 Hive<br>(2)node1上安装Anaconda<br>上传安装包Anaconda3-2021.05-Linux-x86_64.sh<br>sh ./Anaconda3-2021.05-Linux-x86_64.sh</p>
<p>(3)验证是否成功<br>安装完成后, 退出终端重新进来: </p>
<p>看到这个Base开头表明安装好了.base是默认的虚拟环境.<br>(4)安装spark<br>上传压缩包spark-3.2.0-bin-hadoop3.2.tgz<br>解压tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/<br>由于spark目录名称很⻓, 给其一个软链接:<br>ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark<br>(5)配置环境变量<br>添加以下内容：</p>
<p>export SPARK_HOME=/export/server/spark<br>export HADOOP_HOME=/export/server/hadoop-3.3.0<br>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop<br>(6)测试bin /pyspark</p>
<p>输入：sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()</p>
<p>(7)访问WEB UI<br>4040端口是一个WEBUI端口, 可以在浏览器内打开:</p>
<p>6.spark-Standalone配置<br>(1) 集群规划<br>node1\ node2\ node3<br>node1运行: Spark的Master进程  和 1个Worker进程<br>node2运行: spark的1个worker进程<br>node3运行: spark的1个worker进程<br>整个集群提供: 1个master进程 和 3个worker进程<br>(2)安装Anaconda<br>(3)配置workers文件<br>改名, 去掉后面的.template后缀<br>mv workers.template workers<br>编辑worker文件<br>vim workers<br>修改以下内容：<br> 将里面的localhost删除, 追加 node1 node2 node3<br>(4)配置spark-env.sh文件<br>改名 mv spark-env.sh.template spark-env.sh<br>编辑spark-env.sh,<br>添加以下内容：export JAVA_HOME=/export/server/jdk<br>export HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop<br>exportYARN_CONF_DIR=/export/server/hadoop/etc/hadoop</p>
<p>exportSPARK_MASTER_HOST=node1<br>exportSPARK_MASTER_PORT=7077<br>exportSPARK_MASTER_WEBUI_PORT=8080<br>exportSPARK_WORKER_CORES=1<br>exportSPARK_WORKER_MEMORY=1g<br>exportSPARK_WORKER_PORT=7078<br>export SPARK_WORKER_WEBUI_PORT=8081<br>exportSPARK_HISTORY_OPTS=”Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ Dspark.history.fs.cleaner.enabled=true”<br>(5)配置spark-defaults.conf文件<br>改名 mv spark-defaults.conf.template spark-defaults.conf<br>修改内容, 追加如下内容:<br>spark.eventLog.enabled  true<br>spark.eventLog.dir  hdfs://node1:8020/sparklog/<br>spark.eventLog.compress  true<br>(6)配置log4j.properties 文件<br>修改内容 参考下图</p>
<p>(7)将Spark安装文件夹  分发到其它的服务器上<br>scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/<br>scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/<br>(8)在node2和node3上 给spark安装目录增加软链接<br>ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark<br>(9)检查检查每台机器的: JAVA_HOME SPARK_HOME PYSPARK_PYTHON 等等 环境变量是否正常指向正确的目录<br>(10)启动历史服务器<br>sbin/start-history-server.sh<br>(11)启动Spark的Master和Worker进程<br>启动全部master和worker sbin/start-all.sh<br>(12)查看Master的WEB UI</p>
<p>(13)连接到StandAlone集群<br>bin/pyspark –master spark://node1:7077 </p>
<p>7.spark(HA)模式<br>前提: 确保Zookeeper 和 HDFS 均已经启动<br>(1)修改spark-env.sh文件<br>删除SPARK_MASTER_HOST=node1<br>增加以下内容：<br>SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER-Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181-Dspark.d<br>eploy.zookeeper.dir=/spark-ha”<br>(2)将spark-env.sh 分发到每一台服务器上<br>scp spark-env.sh node2:/export/server/spark/conf/<br>scp spark-env.sh node3:/export/server/spark/conf/<br>停止当前StandAlone集群<br>sbin/stop-all.sh<br>(3)启动集群<br>在node1上 启动一个master 和全部worker<br>sbin/start-all.sh<br>下面命令在node2上执行<br>sbin/start-master.sh<br>(4)查看WEB UI<br> <a target="_blank" rel="noopener" href="http://node1:8080/">http://node1:8080/</a><br> <a target="_blank" rel="noopener" href="http://node2:8080/">http://node2:8080/</a><br> 8. spark在yarn上运行<br>(1)角色规划<br>Master角色由YARN的ResourceManager担任.<br>Worker角色由YARN的NodeManager担任.<br>Driver角色运行在YARN容器内或提交任务的客户端进程中<br>真正干活的Executor运行在YARN提供的容器内<br>(2)启动yarn的历史服务器，jps看进程，查看web UI<br>/export/server/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver<br>(3) 在yarn上启动pyspark</p>
<p>(3)测试命令</p>
<p>(4)client模式测试pi</p>
<p>(5)cluster模式测试pi</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/%E3%80%8ASpark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E3%80%8B/index-1.html" data-id="cl3ivshmp0001bsu3arjre8h3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/23/White/">White</a>
          </li>
        
          <li>
            <a href="/2022/05/22/My-New-Post/">My New Post</a>
          </li>
        
          <li>
            <a href="/2022/05/22/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 gaoqi<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>